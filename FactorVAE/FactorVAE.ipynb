{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join, exists\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pixyz.distributions import Normal, Bernoulli, Deterministic\n",
    "from pixyz.losses import KullbackLeibler, CrossEntropy, AdversarialKullbackLeibler, AdversarialWassersteinDistance\n",
    "from pixyz.models import Model\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "batch_size = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_zip = np.load('../data/dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz')\n",
    "imgs = dataset_zip['imgs']\n",
    "imgs = imgs[:,None,:,:].astype(\"float32\")\n",
    "\n",
    "train_imgs, test_imgs = train_test_split(imgs, random_state=42, test_size=1000)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_loader = torch.utils.data.DataLoader(train_imgs, batch_size=128, shuffle=True)  \n",
    "test_loader = torch.utils.data.DataLoader(test_imgs, batch_size=128, shuffle=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 10\n",
    "for gamma in [10, 20]:\n",
    "    # prior model p(z)\n",
    "    loc = torch.tensor(0.).to(device)\n",
    "    scale = torch.tensor(1.).to(device)\n",
    "    prior = Normal(loc=loc, scale=scale, var=[\"z\"], dim=z_dim, name=\"p_prior\")\n",
    "\n",
    "    E = Encoder(z_dim=z_dim).to(device) # q(z|x)\n",
    "    G = Decoder(z_dim=z_dim).to(device) # p(x|z)\n",
    "    D = Discriminator(z_dim=z_dim).to(device) # d(t|z)\n",
    "\n",
    "    class InferenceShuffleDim(Deterministic):\n",
    "        def __init__(self):\n",
    "            super(InferenceShuffleDim, self).__init__(cond_var=[\"x_\"], var=[\"z\"], name=\"q_shuffle\")\n",
    "\n",
    "        def permute_dims(self, z):\n",
    "            B, _ = z.size()\n",
    "            perm_z = []\n",
    "            for z_j in z.split(1, 1):\n",
    "                perm = torch.randperm(B).to(z.device)\n",
    "                perm_z_j = z_j[perm]\n",
    "                perm_z.append(perm_z_j)\n",
    "\n",
    "            return torch.cat(perm_z, 1)        \n",
    "\n",
    "        def forward(self, x_):\n",
    "            z = E.sample({\"x\": x_}, return_all=False)[\"z\"]\n",
    "            return {\"z\": self.permute_dims(z)}\n",
    "\n",
    "    E_shuffle = InferenceShuffleDim()\n",
    "\n",
    "    reconst = CrossEntropy(E, G)\n",
    "    kl = KullbackLeibler(E, prior)\n",
    "    tc = AdversarialKullbackLeibler(E, E_shuffle, discriminator=D, optimizer=optim.Adam, optimizer_params={\"lr\":5e-5})\n",
    "    loss_cls = reconst.mean() + kl.mean() + gamma*tc\n",
    "    model = Model(loss_cls, distributions=[E, G], optimizer=optim.Adam, optimizer_params={\"lr\":1e-4})\n",
    "    print(model)\n",
    "\n",
    "    loss_list = []\n",
    "    loss_d_list = []\n",
    "    for i in range(10):\n",
    "        for batch_idx, x in tqdm(enumerate(train_loader)):\n",
    "            x = x.to(device)\n",
    "            loss = model.train({\"x\": x, \"x_\": x})\n",
    "            loss_d = tc.train({\"x\": x, \"x_\": x})\n",
    "            loss_list.append(loss.detach())\n",
    "            loss_d_list.append(loss_d.detach())\n",
    "        plt.subplot(121)\n",
    "        plt.plot(loss_list)\n",
    "        plt.subplot(122)\n",
    "        plt.plot(loss_d_list)\n",
    "        plt.show()\n",
    "        encoder_plot(test_loader, E, G)\n",
    "        traverse_plot(test_loader, E, G, 1)\n",
    "\n",
    "\n",
    "    log_dir = \"./logs/\"\n",
    "    experiment_name = \"factorvae_dsprites_z_dim{}_gamma{}\".format(z_dim, gamma)\n",
    "    torch.save(E.state_dict(), join(log_dir, 'E_{}.pkl'.format(experiment_name)))\n",
    "    torch.save(D.state_dict(), join(log_dir, 'D_{}.pkl'.format(experiment_name)))\n",
    "    torch.save(G.state_dict(), join(log_dir, 'G_{}.pkl'.format(experiment_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim=10\n",
    "gamma = 10\n",
    "log_dir = \"./logs/\"\n",
    "experiment_name = \"factorvae_dsprites_z_dim{}_gamma{}\".format(z_dim, gamma)\n",
    "E = Encoder(z_dim=z_dim).to(device) # q(z|x)\n",
    "G = Decoder(z_dim=z_dim).to(device) # p(x|z)\n",
    "E.load_state_dict(torch.load(join(log_dir, 'E_{}.pkl'.format(experiment_name))))\n",
    "G.load_state_dict(torch.load(join(log_dir, 'G_{}.pkl'.format(experiment_name))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traverse_plot(test_loader, E, G, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_gif(0, test_loader, E, G, experiment_name, m=30, scale=3, z_dim=10)\n",
    "make_gif(1, test_loader, E, G, experiment_name, m=30, scale=3, z_dim=10)\n",
    "make_gif(2, test_loader, E, G, experiment_name, m=30, scale=3, z_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import models\n",
    "import utils\n",
    "from models import *\n",
    "from utils import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
