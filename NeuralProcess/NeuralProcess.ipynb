{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import linalg as LA\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from os.path import join, exists\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from pixyz.models import Model\n",
    "from pixyz.losses import ELBO, NLL\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "path_gif = \"./NP_png/\"\n",
    "if not exists(path_gif):\n",
    "    os.makedirs(path_gif)\n",
    "    \n",
    "path_gif = \"./ANP_png/\"\n",
    "if not exists(path_gif):\n",
    "    os.makedirs(path_gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データ\n",
    "sigma_y = 0.2\n",
    "\n",
    "N = 8\n",
    "np.random.seed(42)\n",
    "train_X = np.random.uniform(0, 1, N)\n",
    "train_y = np.sin(2 * np.pi * train_X) + np.random.normal(0, sigma_y, N) + 1\n",
    "plt.plot(np.linspace(0,1), np.sin(2 * np.pi * np.linspace(0, 1)) + 1, \"g\")\n",
    "plt.scatter(train_X, np.sin(2 * np.pi * train_X)+1, c=\"b\", label=r\"$f(X)$\") # ノイズがのってないデータ\n",
    "plt.scatter(train_X, train_y, label=r\"$f(X) + \\epsilon$\") # 訓練データ\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ガウス過程\n",
    "\n",
    "sigma_f = 0.5\n",
    "l = 0.2\n",
    "sigma_y = 0.2\n",
    "\n",
    "def k(x, y):\n",
    "    return sigma_f ** 2 * np.exp(- ((x - y) ** 2) / (2 * l ** 2))\n",
    "\n",
    "def k_(x):\n",
    "    return np.vectorize(lambda x, y: k(x, y))(train_X, x)\n",
    "\n",
    "def m(x):\n",
    "    return K_y_inv.dot(train_y).dot(k_(x))\n",
    "\n",
    "def sd(x):\n",
    "    return np.sqrt(k(x, x) - k_(x).dot(K_y_inv).dot(k_(x)))\n",
    "\n",
    "X, Y = np.meshgrid(train_X, train_X)\n",
    "K = np.vectorize(k)(X, Y)  #(N, N) ノイズが乗ってないデータ(f(x))の同時分布の共分散\n",
    "K_y = K + sigma_y ** 2 * np.eye(len(train_X)) #(N, N)  ノイズが乗ったデータ(y)の同時分布の共分散\n",
    "K_y_inv = LA.inv(K_y)\n",
    "\n",
    "Nx = 100\n",
    "x = np.linspace(-0.2, 1.2, Nx)\n",
    "y_mean = np.vectorize(m)(x)\n",
    "y_upper = y_mean + np.vectorize(sd)(x) * 2  # 上2シグマ\n",
    "y_under = y_mean - np.vectorize(sd)(x) * 2 # 下2シグマ\n",
    "\n",
    "plt.scatter(train_X, train_y)\n",
    "plt.plot(x, y_mean, label=r\"$m(x_{N+1})$\")\n",
    "plt.plot(x, y_upper, \"k--\", label=r\"$m(x_{N+1}) + 2\\sigma(x_{N+1})$\")\n",
    "plt.plot(x, y_under, \"k--\")\n",
    "plt.plot(np.linspace(0,1), np.sin(2 * np.pi * np.linspace(0, 1)) + 1, \"g\", alpha=0.5, label=r\"$f(x_{N+1})$\") # f(x)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ\n",
    "z_dim = 3\n",
    "d_dim = 128\n",
    "x_dim = 1\n",
    "y_dim = 1\n",
    "\n",
    "r_encoder = R_encoder(x_dim, y_dim, d_dim, z_dim).to(device) # (x,y)->r\n",
    "s_encoder = S_encoder(x_dim, y_dim, d_dim, z_dim).to(device) # (x,y)->z\n",
    "s_encoder_context = s_encoder.replace_var(x=\"x_c\", y=\"y_c\").to(device)\n",
    "decoder = Decoder(x_dim, y_dim, d_dim, z_dim).to(device) # (x*, r, z) -> y*\n",
    "opt = torch.optim.Adam(list(decoder.parameters())+list(r_encoder.parameters())+\n",
    "                       list(s_encoder.parameters()), 1e-3)\n",
    "\n",
    "#%debug\n",
    "x_grid = torch.from_numpy(np.arange(-4, 4, 0.1).reshape(-1,1).astype(np.float32)).to(device)\n",
    "for i in range(15):\n",
    "    untrained_zs = torch.from_numpy(np.random.normal(size=(z_dim)).astype(np.float32)).to(device)\n",
    "    mu = decoder.sample_mean({\"x_\": x_grid, \"r\": untrained_zs.repeat(len(x_grid), 1), \"z\": untrained_zs.repeat(len(x_grid), 1)})\n",
    "    plt.plot(x_grid.cpu().data.numpy(), mu.cpu().data.numpy(), linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X_ = torch.from_numpy(train_X.astype(\"float32\")).to(device)\n",
    "train_y_ = torch.from_numpy(train_y.astype(\"float32\")).to(device)\n",
    "\n",
    "# 訓練\n",
    "for i in range(20000):\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # context target split\n",
    "    x_c, x_t, y_c, y_t = context_target_random_split(train_X_[:, None], train_y_[:, None])\n",
    "    x_ct = torch.cat([x_c, x_t], dim=0)\n",
    "    y_ct = torch.cat([y_c, y_t], dim=0)\n",
    "\n",
    "    # deterministic path\n",
    "    r_mean = r_encoder(x_c, y_c)[\"r\"].mean(0) # aggregate\n",
    "\n",
    "    # latent path\n",
    "    z_sample_target = s_encoder.sample({\"x\": x_t, \"y\": y_t})\n",
    "    \n",
    "    # Loss\n",
    "    nll = - decoder.log_likelihood({\"x_\": x_t, \"r\": r_mean.repeat(len(x_t), 1), \"z\": z_sample_target[\"z\"].repeat(len(x_t), 1), \"y_\": y_t})\n",
    "    kl = s_encoder.log_likelihood(z_sample_target) - s_encoder_context.log_likelihood({\"x_c\": x_c, \"y_c\": y_c, \"z\": z_sample_target[\"z\"]})\n",
    "    loss = nll.mean() + kl.mean()\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "\n",
    "    # visualize\n",
    "    if ((i+1)%200)==0:\n",
    "        Nx = 100\n",
    "        x = np.linspace(-0.2, 1.2, Nx)\n",
    "        x_ = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
    "        r_mean = r_encoder(x_ct, y_ct)[\"r\"].mean(0)\n",
    "        for j in range(10):\n",
    "            z = s_encoder.sample({\"x\": x_ct, \"y\": y_ct})[\"z\"]\n",
    "            y_ = decoder.sample_mean({\"x_\": x_[:, None], \"r\": r_mean.repeat(len(x_), 1), \"z\": z.repeat(len(x_), 1)})\n",
    "            y_ = y_.detach().cpu().numpy()\n",
    "            if j == 0:\n",
    "                plt.plot(x, y_, alpha=0.5, c=\"b\",label=\"NP sample\")\n",
    "            else:\n",
    "                plt.plot(x, y_, alpha=0.5, c=\"b\")\n",
    "\n",
    "\n",
    "        plt.scatter(train_X, train_y)\n",
    "        plt.title(\"epoch: {}\".format(i+1))\n",
    "        plt.xlim(-0.22, 1.22)\n",
    "        plt.ylim(-0.55, 2.4)\n",
    "        plt.plot(x, y_mean, \"k\", label=\"GP mean\")\n",
    "        plt.plot(x, y_upper, \"k--\", label=\"GP 2sigma\")\n",
    "        plt.plot(x, y_under, \"k--\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.savefig(\"./NP_png/{}\".format(i+1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 2\n",
    "d_dim = 128\n",
    "x_dim = 1\n",
    "y_dim = 1\n",
    "\n",
    "r_encoder = R_encoder(x_dim, y_dim, d_dim, z_dim).to(device) # (x,y)->r\n",
    "s_encoder = S_encoder(x_dim, y_dim, d_dim, z_dim).to(device) # (x,y)->z\n",
    "s_encoder_context = s_encoder.replace_var(x=\"x_c\", y=\"y_c\").to(device)\n",
    "CA = CrossAttention(x_dim, d_dim, z_dim).to(device) # (x_t, x_c, r)->r_\n",
    "decoder = Decoder(x_dim, y_dim, d_dim, z_dim).to(device) # (x*, z) -> y*\n",
    "opt = torch.optim.Adam(list(decoder.parameters())+list(r_encoder.parameters())+\n",
    "                       list(s_encoder.parameters())+list(CA.parameters()), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X_ = torch.from_numpy(train_X.astype(\"float32\")).to(device)\n",
    "train_y_ = torch.from_numpy(train_y.astype(\"float32\")).to(device)\n",
    "\n",
    "# 訓練\n",
    "for epoch in range(20000):\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # context target split\n",
    "    x_c, x_t, y_c, y_t = context_target_random_split(train_X_[:, None], train_y_[:, None])\n",
    "    x_ct = torch.cat([x_c, x_t], dim=0)\n",
    "    y_ct = torch.cat([y_c, y_t], dim=0)\n",
    "\n",
    "    # deterministic path\n",
    "    r = r_encoder(x_c, y_c)[\"r\"]\n",
    "    r = CA(x_t, x_c, r)[\"r_\"]  # ATTENTION\n",
    "    \n",
    "    z_sample_target = s_encoder.sample({\"x\": x_t, \"y\": y_t})\n",
    "\n",
    "    # Loss\n",
    "    nll = - decoder.log_likelihood({\"x_\": x_t, \"r\": r, \"z\": z_sample_target[\"z\"].repeat(len(x_t), 1), \"y_\": y_t})\n",
    "    kl = s_encoder.log_likelihood(z_sample_target) - s_encoder_context.log_likelihood({\"x_c\": x_c, \"y_c\": y_c, \"z\": z_sample_target[\"z\"]})\n",
    "    loss = nll.mean() + kl.mean()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # visualize\n",
    "    if ((epoch+1)%200)==0:\n",
    "        Nx = 100\n",
    "        x = np.linspace(-0.2, 1.2, Nx)\n",
    "        x_ = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
    "        r = r_encoder(x_ct, y_ct)[\"r\"]\n",
    "        r = CA(x_[:, None], x_ct, r)[\"r_\"]\n",
    "        for j in range(10):\n",
    "            z = s_encoder.sample({\"x\": x_ct, \"y\": y_ct})[\"z\"]\n",
    "            x_ = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
    "            y_ = decoder.sample_mean({\"x_\": x_[:, None], \"r\": r, \"z\": z.repeat(len(x_), 1)})\n",
    "            y_ = y_.detach().cpu().numpy()\n",
    "            if j == 0:\n",
    "                plt.plot(x, y_, alpha=0.5, c=\"b\",label=\"ANP sample\")\n",
    "            else:\n",
    "                plt.plot(x, y_, alpha=0.5, c=\"b\")\n",
    "\n",
    "\n",
    "        plt.scatter(train_X, train_y)\n",
    "        plt.title(\"epoch: {}\".format(epoch+1))\n",
    "        plt.xlim(-0.22, 1.22)\n",
    "        plt.ylim(-0.55, 2.4)\n",
    "        plt.plot(x, y_mean, \"k\", label=\"GP mean\")\n",
    "        plt.plot(x, y_upper, \"k--\", label=\"GP 2sigma\")\n",
    "        plt.plot(x, y_under, \"k--\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.savefig(\"./ANP_png/{}\".format(epoch+1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
