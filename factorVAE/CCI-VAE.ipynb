{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "X0fWovq1E7T_"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join, exists\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pixyz.distributions import Normal, Bernoulli, Deterministic\n",
    "from pixyz.losses import KullbackLeibler, CrossEntropy, AdversarialKullbackLeibler, Parameter\n",
    "from pixyz.models import Model\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "batch_size = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_zip = np.load('../data/dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz')\n",
    "imgs = dataset_zip['imgs']\n",
    "imgs = imgs[:,None,:,:].astype(\"float32\")\n",
    "\n",
    "train_imgs, test_imgs = train_test_split(imgs, random_state=42, test_size=1000)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_loader = torch.utils.data.DataLoader(train_imgs, batch_size=128, shuffle=True)  \n",
    "test_loader = torch.utils.data.DataLoader(test_imgs, batch_size=128, shuffle=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2576it [01:06, 38.83it/s]"
     ]
    }
   ],
   "source": [
    "for gamma in [100, 500]:\n",
    "    z_dim=10\n",
    "    epoch_num = 15\n",
    "    N = len(train_loader)\n",
    "\n",
    "    # prior model p(z)\n",
    "    loc = torch.tensor(0.).to(device)\n",
    "    scale = torch.tensor(1.).to(device)\n",
    "    prior = Normal(loc=loc, scale=scale, var=[\"z\"], dim=z_dim, name=\"p_prior\")\n",
    "\n",
    "    E = Encoder(z_dim=z_dim).to(device) # q(z|x)\n",
    "    D = Decoder(z_dim=z_dim).to(device) # p(x|z)\n",
    "\n",
    "    reconst = CrossEntropy(E, D)\n",
    "    kl = KullbackLeibler(E, prior)\n",
    "    C = Parameter(\"C\")\n",
    "    loss_cls = reconst.mean() + gamma*(kl.mean()-C).abs()\n",
    "    model = Model(loss_cls, distributions=[E, D], optimizer=optim.Adam, optimizer_params={\"lr\":5e-4})\n",
    "\n",
    "    loss_list = []\n",
    "    for epoch in range(epoch_num):\n",
    "        for batch_idx, x in tqdm(enumerate(train_loader)):\n",
    "            x = x.to(device)\n",
    "            C_ = 25*(batch_idx+epoch*N)/(epoch_num*N)\n",
    "            loss = model.train({\"x\": x, \"C\": C_})\n",
    "            loss_list.append(loss.detach())\n",
    "        plt.plot(loss_list)\n",
    "        plt.show()\n",
    "        encoder_plot(test_loader, E, D)\n",
    "        traverse_plot(test_loader, E, D, 1)\n",
    "\n",
    "\n",
    "    log_dir = \"./logs/\"\n",
    "    experiment_name = \"betavae_C_dsprites_z_dim{}_gamma{}\".format(z_dim, gamma)\n",
    "    torch.save(E.state_dict(), join(log_dir, 'E_{}.pkl'.format(experiment_name)))\n",
    "    torch.save(D.state_dict(), join(log_dir, 'D_{}.pkl'.format(experiment_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim=10\n",
    "gamma = 100\n",
    "\n",
    "log_dir = \"./logs/\"\n",
    "experiment_name = \"betavae_C_dsprites_z_dim{}_gamma{}\".format(z_dim, beta)\n",
    "E = Encoder(z_dim=z_dim).to(device) # q(z|x)\n",
    "D = Decoder(z_dim=z_dim).to(device) # p(x|z)\n",
    "E.load_state_dict(torch.load(join(log_dir, 'E_{}.pkl'.format(experiment_name))))\n",
    "D.load_state_dict(torch.load(join(log_dir, 'D_{}.pkl'.format(experiment_name))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traverse_plot(test_loader, E, D, 1, scale=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gifをつくる  \n",
    "0: 四角  \n",
    "1: 楕円  \n",
    "2: ハート  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_gif(0, test_loader, E, D, experiment_name, m=30, scale=3, z_dim=10)\n",
    "make_gif(1, test_loader, E, D, experiment_name, m=30, scale=3, z_dim=10)\n",
    "make_gif(2, test_loader, E, D, experiment_name, m=30, scale=3, z_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import models\n",
    "import utils\n",
    "reload(models)\n",
    "reload(utils)\n",
    "from models import *\n",
    "from utils import *"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "180221-variational-autoencoder.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
