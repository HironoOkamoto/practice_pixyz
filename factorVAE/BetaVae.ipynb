{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "X0fWovq1E7T_"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join, exists\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pixyz.distributions import Normal, Bernoulli, Deterministic\n",
    "from pixyz.losses import KullbackLeibler, CrossEntropy, AdversarialKullbackLeibler\n",
    "from pixyz.models import Model\n",
    "\n",
    "from models import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JVDd2jN2E9Fw"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_zip = np.load('../data/dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz')\n",
    "imgs = dataset_zip['imgs']\n",
    "imgs = imgs[:,None,:,:].astype(\"float32\")\n",
    "\n",
    "train_imgs, test_imgs = train_test_split(imgs, random_state=42, test_size=1000)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_loader = torch.utils.data.DataLoader(train_imgs, batch_size=128, shuffle=True)  \n",
    "test_loader = torch.utils.data.DataLoader(test_imgs, batch_size=128, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim=10\n",
    "beta = 50\n",
    "\n",
    "# prior model p(z)\n",
    "loc = torch.tensor(0.).to(device)\n",
    "scale = torch.tensor(1.).to(device)\n",
    "prior = Normal(loc=loc, scale=scale, var=[\"z\"], dim=z_dim, name=\"p_prior\")\n",
    "\n",
    "E = Encoder(z_dim=z_dim).to(device) # q(z|x)\n",
    "D = Decoder(z_dim=z_dim).to(device) # p(x|z)\n",
    "\n",
    "reconst = CrossEntropy(E, D)\n",
    "kl = KullbackLeibler(E, prior)\n",
    "loss_cls = reconst.mean() + beta*kl.mean()\n",
    "model = Model(loss_cls, distributions=[E, D], optimizer=optim.Adam, optimizer_params={\"lr\":5e-4})\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "for i in range(5):\n",
    "    for batch_idx, x in tqdm(enumerate(train_loader)):\n",
    "        x = x.to(device)\n",
    "        loss = model.train({\"x\": x})\n",
    "        loss_list.append(loss.detach())\n",
    "    plt.plot(loss_list)\n",
    "    plt.show()\n",
    "    encoder_plot(test_loader, E, D)\n",
    "    traverse_plot(test_loader, E, D, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./logs/\"\n",
    "experiment_name = \"betavae_dsprites_disentangle_z_dim{}_beta{}\".format(z_dim, beta)\n",
    "torch.save(E.state_dict(), join(log_dir, 'E_{}.pkl'.format(experiment_name)))\n",
    "torch.save(D.state_dict(), join(log_dir, 'D_{}.pkl'.format(experiment_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./logs/\"\n",
    "experiment_name = \"betavae_dsprites_disentangle_z_dim{}_beta{}\".format(z_dim, beta)\n",
    "E.load_state_dict(torch.load(join(log_dir, 'E_{}.pkl'.format(experiment_name))))\n",
    "D.load_state_dict(torch.load(join(log_dir, 'D_{}.pkl'.format(experiment_name))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traverse_plot(test_loader, E, D, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import models\n",
    "import utils\n",
    "reload(models)\n",
    "reload(utils)\n",
    "from models import *\n",
    "from utils import *"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "180221-variational-autoencoder.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
